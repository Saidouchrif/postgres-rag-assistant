# postgres-course-rag-assistant

Assistant RAG multimodal qui rÃ©pond aux questions sur des cours universitaires en s'appuyant sur une base PostgreSQL + pgvector et les API OpenAI. Le projet inclut un pipeline d'ingestion, un backend FastAPI et une interface Streamlit pour le chat.

## ğŸš€ FonctionnalitÃ©s clÃ©s

- **RÃ©cupÃ©ration augmentÃ©e** Ã  partir d'un corpus de PDF et d'images indexÃ©s.
- **Embeddings OpenAI** pour la similaritÃ© vectorielle avec pgvector.
- **API FastAPI** exposant un endpoint `/chat` consommÃ© par la web app.
- **Interface Streamlit** pour poser des questions et visualiser les sources.
- **Pipeline d'ingestion** automatisÃ© pour PDF et images (captioning + embeddings).


### Diagramme de communication (requÃªte `/chat`)

```mermaid
sequenceDiagram
    participant U as Utilisateur
    participant UI as Streamlit UI
    participant API as Backend FastAPI
    participant RAG as RAG Core
    participant DB as PostgreSQL + pgvector
    participant LLM as OpenAI APIs

    U->>UI: Pose une question
    UI->>API: POST /chat {question, k}
    API->>RAG: PrÃ©pare la requÃªte de recherche
    RAG->>DB: Recherche des documents top-k similaires
    DB-->>RAG: Documents pertinents + scores
    RAG->>LLM: Prompt + contexte (documents)
    LLM-->>RAG: RÃ©ponse augmentÃ©e
    RAG-->>API: RÃ©ponse + mÃ©ta-donnÃ©es (sources)
    API-->>UI: JSON rÃ©ponse + sources
    UI-->>U: Affichage de la rÃ©ponse et des sources
```

### Diagramme de communication (ingestion)

```mermaid
sequenceDiagram
    participant CLI as Script d'ingestion
    participant FS as Fichiers (PDF / Images)
    participant LLM as OpenAI APIs
    participant DB as PostgreSQL + pgvector

    CLI->>FS: Lecture des documents
    CLI->>LLM: Captioning / Embeddings
    LLM-->>CLI: Vecteurs et textes enrichis
    CLI->>DB: INSERT documents + embeddings
```

## ğŸ—‚ï¸ Structure du projet

```
postgres-course-rag-assistant/
â”œâ”€â”€ backend/             # API FastAPI (endpoints, modÃ¨les Pydantic)
â”œâ”€â”€ database/            # Connexion PostgreSQL + pgvector
â”œâ”€â”€ frontend/            # Interface Streamlit
â”œâ”€â”€ images/              # Captures d'Ã©cran pour la documentation
â”œâ”€â”€ ingestion/           # Script d'ingestion PDF & images
â”œâ”€â”€ utils/               # Utilitaires OpenAI (embeddings, captioning)
â”œâ”€â”€ data/                # Documents sources Ã  indexer (PDF, images)
â”œâ”€â”€ docker-compose.yml   # Conteneur PostgreSQL + pgvector
â””â”€â”€ README.md
```

## âš™ï¸ PrÃ©requis

- Python 3.11+
- [Poetry](https://python-poetry.org/) ou `pip` classique
- Compte OpenAI + clÃ© API valide (`OPENAI_API_KEY`)
- Docker & Docker Compose pour pgvector

## ğŸ› ï¸ Installation & Configuration

1. **Cloner le dÃ©pÃ´t**
   ```bash
   git clone https://github.com/votre-org/postgres-course-rag-assistant.git
   cd postgres-course-rag-assistant
   ```

2. **Configurer l'environnement**
   - Copier `.env.example` vers `.env` (ou crÃ©er `.env`) et renseigner :

     | Variable        | Description                                  |
     |-----------------|----------------------------------------------|
     | `OPENAI_API_KEY`| ClÃ© API OpenAI                               |
     | `PG_HOST`       | HÃ´te PostgreSQL (ex. `localhost`)            |
     | `PG_PORT`       | Port PostgreSQL (ex. `5432`)                 |
     | `PG_DB`         | Base de donnÃ©es (ex. `ragdb`)                |
     | `PG_USER`       | Utilisateur PostgreSQL (ex. `raguser`)       |
     | `PG_PASSWORD`   | Mot de passe PostgreSQL (ex. `ragpass`)      |

3. **Installer les dÃ©pendances Python**
   ```bash
   python -m venv .venv
   .venv\Scripts\activate  # sous Windows
   pip install -r requirements.txt
   pip install -r backend/requirements.txt
   ```

4. **Lancer PostgreSQL + pgvector**
   ```bash
   docker compose up -d
   ```

5. **Initialiser la base** (assurez-vous que la table `documents` existe avec une colonne `embedding` de type `vector`).

## ğŸ“¥ Ingestion des donnÃ©es

1. DÃ©posez vos PDF et images dans le dossier `data/`.
2. Lancez le script d'ingestion :
   ```bash
   python ingestion/ingest.py
   ```
   - Les PDF sont dÃ©coupÃ©s en chunks (800 tokens) avec chevauchement.
   - Les images sont sous-titrÃ©es via OpenAI avant vectorisation.
3. VÃ©rifiez que les documents sont insÃ©rÃ©s dans la table `documents`.

## â–¶ï¸ Lancement des services

1. **Backend FastAPI**
   ```bash
   uvicorn backend.main:app --reload --port 8000
   ```
   Endpoint principal : `POST /chat`

2. **Frontend Streamlit**
   ```bash
   streamlit run frontend/app.py
   ```
   Ouvrez [http://localhost:8501](http://localhost:8501) pour utiliser l'interface.

## ğŸ“¡ API

- `GET /health` : vÃ©rifie l'Ã©tat du service.
- `POST /chat` : corps JSON `{ "question": "...", "k": 5 }`
  - Retourne `answer` (texte) et `sources` (liste des chunks, sources et scores).

## ğŸ” DÃ©pannage

- **Erreur de connexion PG** : vÃ©rifiez l'Ã©tat du conteneur docker et les variables `PG_*`.
- **RÃ©ponse vide** : assurez-vous que le script d'ingestion a Ã©tÃ© lancÃ© et que l'API OpenAI est accessible.
- **Quota OpenAI** : surveillez l'utilisation, les embeddings et captionings consomment des crÃ©dits.

## ğŸ“¸ Captures d'Ã©cran

| UI | Description |
|----|-------------|
| ![Recherche](images/scrin1.png) | Ã‰cran principal Streamlit avec zone de question. |
| ![RÃ©sultats](images/scrin2.png) | Affichage de la rÃ©ponse gÃ©nÃ©rÃ©e. |
| ![Sources](images/scrin3.png) | DÃ©tails des sources et scores par chunk. |

## ğŸ¤ Contributions

Les contributions (issues, PR) sont les bienvenues ! Pensez Ã  exÃ©cuter le linting et Ã  documenter vos modifications.

## ğŸ“„ Licence

Ce projet est proposÃ© sous licence MIT (adapter selon vos besoins).
